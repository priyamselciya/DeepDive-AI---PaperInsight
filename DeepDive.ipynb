{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key\n",
    "openai.api_key = \"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\"  # Replace with your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade416836ac34fd5a7d67bbc90e3542e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivakumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sivakumar\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacbe5f1372e46e2a697216a10314001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acb90d4e1894cb48a0901b9645fdbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884bcef18fcf44b697449d7eeee11ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2e4204d60641d88ffaeb25f7fc3e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59986b06563343bebcf51efe239912a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ead0b75ec6441ca77b8e5ff89f19bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828e6837932547d9b47b28f98c587a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b92c0f1d4694e2196f711ef3a058dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1597f2e14e42d8a8b22af0b5458b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0c5c4cb4d0469eab9cfe6c40dcd3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Sentence Transformer model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAISS index\n",
    "dimension = 384  # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ **Text from D:\\project\\1706.03762v7.pdf:**\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez‚àó ‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu≈Åukasz Kaiser‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó ‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experime\n",
      "\n",
      "\n",
      "üìÇ **Text from D:\\project\\2501.12948v1.pdf:**\n",
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
      "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\n",
      "vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\n",
      "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
      "reasoning behaviors. However, it encounters challenges such as poor readability, and language\n",
      "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
      "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\n",
      "R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\n",
      "research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n",
      "(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based o\n",
      "\n",
      "\n",
      "üìÇ **Text from D:\\project\\2312.11805v4.pdf:**\n",
      "Gemini: A Family of Highly Capable\n",
      "Multimodal Models\n",
      "Gemini Team, Google1\n",
      "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\n",
      "across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\n",
      "sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\n",
      "use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\n",
      "advances the state of the art in 30 of 32 of these benchmarks ‚Äî notably being the first model to achieve\n",
      "human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the\n",
      "art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of\n",
      "the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of\n",
      "use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to\n",
      "users through s\n",
      "\n",
      "\n",
      "üìÇ **Text from D:\\project\\2407.21783v3.pdf:**\n",
      "The Llama 3 Herd of Models\n",
      "Llama Team, AI @ Meta1\n",
      "1A detailed contributor list can be found in the appendix of this paper.\n",
      "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a\n",
      "new set of foundation models, called Llama 3. It is a herd of language models that natively support\n",
      "multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with\n",
      "405B parameters and a context window of up to 128K tokens. This paper presents an extensive\n",
      "empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language\n",
      "models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and\n",
      "post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input\n",
      "and output safety. The paper also presents the results of experiments in which we integrate image,\n",
      "video, and speech capabilities into Llama 3 via a compositional approach. We observe this\n",
      "\n",
      "\n",
      "üìÇ **Text from D:\\project\\2303.08774v6.pdf:**\n",
      "GPT-4 Technical Report\n",
      "OpenAI‚àó\n",
      "Abstract\n",
      "We report the development of GPT-4, a large-scale, multimodal model which can\n",
      "accept image and text inputs and produce text outputs. While less capable than\n",
      "humans in many real-world scenarios, GPT-4 exhibits human-level performance\n",
      "on various professional and academic benchmarks, including passing a simulated\n",
      "bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\n",
      "based model pre-trained to predict the next token in a document. The post-training\n",
      "alignment process results in improved performance on measures of factuality and\n",
      "adherence to desired behavior. A core component of this project was developing\n",
      "infrastructure and optimization methods that behave predictably across a wide\n",
      "range of scales. This allowed us to accurately predict some aspects of GPT-4‚Äôs\n",
      "performance based on models trained with no more than 1/1,000th the compute of\n",
      "GPT-4.\n",
      "1 Introduction\n",
      "This technical report presents GPT-4, a large multimodal model capa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdfs(file_paths):\n",
    "    \"\"\"Extracts text from multiple PDF files in a local environment.\"\"\"\n",
    "    extracted_text = {}\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if not os.path.exists(file_path):  # Check if file exists\n",
    "            extracted_text[file_path] = f\"‚ùå File not found -> {file_path}\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\\n\".join([page.extract_text() for page in pdf_reader.pages if page.extract_text()])\n",
    "                extracted_text[file_path] = text[:1000]  # Limit output to first 1000 chars for preview\n",
    "        except Exception as e:\n",
    "            extracted_text[file_path] = f\"‚ö†Ô∏è Error reading {file_path}: {str(e)}\"\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "# ‚úÖ **Fixed file path list with commas and raw strings**\n",
    "pdf_files = [\n",
    "    r\"D:\\project\\1706.03762v7.pdf\",\n",
    "    r\"D:\\project\\2501.12948v1.pdf\",\n",
    "    r\"D:\\project\\2312.11805v4.pdf\",\n",
    "    r\"D:\\project\\2407.21783v3.pdf\",\n",
    "    r\"D:\\project\\2303.08774v6.pdf\"\n",
    "]  # Replace with actual file paths\n",
    "\n",
    "# üîç Extract text from PDFs\n",
    "pdf_texts = extract_text_from_pdfs(pdf_files)\n",
    "\n",
    "# üìÑ Print the extracted text for each file\n",
    "for file, text in pdf_texts.items():\n",
    "    print(f\"\\nüìÇ **Text from {file}:**\\n{text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text into chunks\n",
    "def preprocess_text(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings and build FAISS index\n",
    "def build_faiss_index(chunks):\n",
    "    chunk_embeddings = model.encode(chunks)\n",
    "    index.add(np.array(chunk_embeddings))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform semantic search\n",
    "def semantic_search(query, index, chunks, k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), k)\n",
    "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response using OpenAI GPT-3.5\n",
    "def generate_response(query, context):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
